{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics\n",
    "\n",
    "Goal here is to develop three different accuracy metrics for the EEG online decoder. The three accuracy metrics are described as follows:\n",
    "\n",
    "**1. Fraction of time target is acquired:** The decoder will move in the direction of the on screen targets. Eventually the decoer will saturate and be at the same location as the on-screen target. This method will compute accuracy as the fraction of time the decoders position is equal to the target position (divided by the fraction of time the decoder is not at the center of the screen). The decoder is assumed to be inactive when it is at the center of the screen so these timesteps are omitted from the accuracy calculations\n",
    "\n",
    "**2. Fraction of time decoder is in the correct direction:** This computes the fraction of timesteps the decoder is closest to the correct target.\n",
    "\n",
    "**3. Fraction of time decoder has the correct velocity:** This computes the velocity of the decoder at every timestep. The fraction of these timesteps where the velocity is in the direction of the correct target (relative to all active timesteps) is then reported as accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import sys\n",
    "# setting path\n",
    "sys.path.append('/home/brandon/eeg/Offline_EEGNet')\n",
    "\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import preprocessing as pre\n",
    "from preprocessing import load_data, parse_labels, partition_data, augment_data\n",
    "from train import train\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import torch\n",
    "import pdb\n",
    "from EEGNet import EEGNet\n",
    "import argparse\n",
    "import tqdm\n",
    "from collections import defaultdict\n",
    "import data.data_util as util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load in data to debug the three accuracy metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "data_file_name = \"BM_4_movement_w_feedback_000\"\n",
    "fpath = \"/home/brandon/eeg/bci_raspy/data/\" + data_file_name\n",
    "eeg_data = util.load_data(fpath + \"/eeg.bin\")\n",
    "task_data = util.load_data(fpath + \"/task.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eeg_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(task_data[\"decoded_pos\"][:,0])   # (t_steps, 2)  each t_step contains (x, y) coordinate of decoder\n",
    "plt.plot(task_data[\"state_task\"])\n",
    "plt.legend([\"decoder x-position\", \"state task\"])\n",
    "task_data[\"decoded_pos\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into trials\n",
    "# extract trial start times\n",
    "new_state_bool = np.zeros(task_data['state_task'].size, dtype='bool')\n",
    "new_state_bool[1:] = (task_data['state_task'][1:] != task_data['state_task'][:-1])\n",
    "new_state_inds = np.nonzero(new_state_bool)[0]\n",
    "trial_labels = task_data['state_task'][new_state_inds]\n",
    "trial_start_times = task_data['time_ns'][new_state_inds] \n",
    "eeg_timing = eeg_data['time_ns']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eeg_trials = defaultdict(lambda: [])  \n",
    "# partition trials\n",
    "for trial_ix in range(len(trial_labels)-1):\n",
    "    if int(trial_labels[trial_ix]) == 4:  # skip rest trials\n",
    "        continue \n",
    "\n",
    "    # extract eeg_data indices were trial started and ended\n",
    "    trial_start_ix = np.where(task_data[\"time_ns\"] > trial_start_times[trial_ix])[0][0] + 50 # omit first second of data\n",
    "    trial_end_ix = np.where(task_data[\"time_ns\"] < trial_start_times[trial_ix+1])[0][-1]\n",
    "    \n",
    "    # get the decoder position\n",
    "    eeg_trials[trial_labels[trial_ix]].append(task_data[\"decoded_pos\"][trial_start_ix:trial_end_ix])\n",
    "\n",
    "    # append data from 64 electrodes\n",
    "    #eeg_trials.append(eeg_data['databuffer'][trial_start_ix:trial_end_ix, :64])\n",
    "    #min_trial_len = np.minimum(eeg_trials[-1].shape[0], min_trial_len).astype(int)\n",
    "    #eeg_trial_labels.append(class_names[trial_labels[trial_ix]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### eeg_trials\n",
    "This is a dictionary where the keys are the trial labels (0 for left, 1 for right). The dictionary contains a list of all trials with the corresponding label (with the first second omitted). The trial is formatted as (t_steps, 2) where at each t_step the x,y-coordinate of the deocder is stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_state_inds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'X-position of Decoder During right trial')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "sample_trial_ix = 6  # random trial to visualize\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eeg_trials[0][sample_trial_ix][:,0])\n",
    "plt.title(\"X-position of Decoder During left trial\")\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(eeg_trials[1][sample_trial_ix][:,0])\n",
    "plt.title(\"X-position of Decoder During right trial\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmp = np.clip(eeg_trials[0], -30, 30)\n",
    "tmp == [3, 3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of targets: 4\n",
      "targets: [0, 1, 2, 3]\n"
     ]
    }
   ],
   "source": [
    "targets = np.unique(task_data[\"state_task\"]) \n",
    "targets = list(filter(lambda x: x != 4, targets))  # remove resting state\n",
    "n_targets = len(targets)\n",
    "print(\"number of targets:\", n_targets)\n",
    "print(\"targets:\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## correct if target was reached otherwise take endpoint as decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_correct_trials(decoder_positions, target, method=\"near\"):\n",
    "    '''\n",
    "    returns the number of total and correct trials\n",
    "    \n",
    "    decoder_positions is a numpy array with shape \n",
    "    (n_trials, t_steps, 2) that contians the x and \n",
    "    y positions predicted by the decoder for each \n",
    "    timestep of each trial. decoder_positions must \n",
    "    only contain trials of the same type (e.g. left \n",
    "    reach trials). \n",
    "    \n",
    "    The common label/target for these trials is given \n",
    "    by target position. \n",
    "    \n",
    "    method can be \"near\" or \"hit\". If \"near\" a trial that hits \n",
    "    the correct target or ends near the correct target will \n",
    "    be considered correct. If \"hit\" only trials that actually \n",
    "    hit the correct target are considered correct.\n",
    "    \n",
    "    NOTE: this is only implemented for a one dimensional \n",
    "    decoder using the x-position\n",
    "    \n",
    "    '''\n",
    "    n_trials = decoder_positions.shape[0]\n",
    "    t_steps = decoder_positions.shape[1]\n",
    "    n_correct = 0 \n",
    "    \n",
    "    target = target[0]  # only consider one dimensional task\n",
    "    \n",
    "    # loop over trials\n",
    "    for trial_ix in range(n_trials):\n",
    "        # loop over timesteps in current trial\n",
    "        n_correct += is_trial_correct(decoder_positions[trial_ix], target, method=method)\n",
    "    # end loop over trials\n",
    "    return n_correct, n_trials\n",
    "\n",
    "def is_trial_correct(decoder_positions, target, method=\"near\"):\n",
    "    t_steps = decoder_positions.shape[0]\n",
    "    # loop over timesteps\n",
    "    for t_step in range(t_steps):\n",
    "        # the target was hit\n",
    "        if decoder_positions[t_step, 0] == target:\n",
    "            #print(\"target was acquired\")\n",
    "            return True\n",
    "        # the wrong target was hit\n",
    "        if decoder_positions[t_step, 0] == -target:\n",
    "            #print(\"wrong target was acquired\")\n",
    "            return False    # don't consider future timesteps in this trial\n",
    "    # end loop over timesteps\n",
    "    # the target was never hit\n",
    "    # take the final position as the decision\n",
    "    if np.sign(decoder_positions[t_step, 0]) == np.sign(target) and method==\"near\":\n",
    "        # decoder position was closest to (had the same sign) as the\n",
    "        # correct target position\n",
    "        #print(\"trial ended with decoder near target\")\n",
    "        return True   # count this is a correct trial\n",
    "    return False  # target wasn't hit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_pos[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_thresh = 75  # value at which decoder reaches target\n",
    "\n",
    "# maps target to decoder position\n",
    "target2pos = {\n",
    "    0 : [-decoder_thresh, 0],  # left\n",
    "    1 : [decoder_thresh, 0],   # right\n",
    "    2 : [0, -decoder_thresh],  # down\n",
    "    3 : [0, decoder_thresh]    # up\n",
    "}\n",
    "\n",
    "# stores the total number of trials for each target type\n",
    "n_trials = defaultdict(lambda: -1)\n",
    "# stores the number of correct trials for each target type\n",
    "n_correct = defaultdict(lambda: -1)\n",
    "\n",
    "# holds thresholded the decoder position\n",
    "decoder_pos = defaultdict(lambda: 0)\n",
    "\n",
    "# compute per-class accuracy\n",
    "verbose = True  # prints intermediate values\n",
    "decoder_acc = defaultdict(lambda: 0)\n",
    "for target in targets:\n",
    "    decoder_pos[target] = np.clip(eeg_trials[target], -decoder_thresh, decoder_thresh)\n",
    "    n_correct[target], n_trials[target] = count_correct_trials(decoder_pos[target], target2pos[target], method=\"hit\")\n",
    "    #correct_steps = decoder_pos[target] == target2pos[target]\n",
    "    #correct_steps = np.sum(correct_steps, axis=-1)\n",
    "    #correct_steps = np.where(correct_steps==2)[0].shape[0]\n",
    "    #assert False\n",
    "    \n",
    "    \n",
    "    #correct_steps = decoder_pos == target2pos[target]\n",
    "    #correct_steps = np.sum(correct_steps, axis=-1)\n",
    "    #correct_steps = np.where(correct_steps==2)[0].shape[0]\n",
    "    \n",
    "    #n_trials = decoder_pos[target].shape[0] \n",
    "    #n_tsteps_per_trial = decoder_pos[target].shape[1]\n",
    "    #n_active_steps[target] = n_trials * n_tsteps_per_trial\n",
    "    \n",
    "    acc = n_correct[target]*100./n_trials[target]\n",
    "    decoder_acc[target] = np.maximum(0., acc)  # negative accuracy indicates unknown class\n",
    "    if verbose:\n",
    "        print(\"\\nTarget:\", target)\n",
    "        print(\"total number of steps during this target:\", n_trials[target])\n",
    "        print(\"number of correct steps:\")\n",
    "        print(\"accuracy(%):\", acc )\n",
    "    \n",
    "# decoder_acc[target_ix] = accuracy_of_decoder_for_that_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute combined decoder accuracy\n",
    "numerator = 0\n",
    "denominator = 1e-12\n",
    "for target in targets:\n",
    "    numerator += n_correct[target]\n",
    "    denominator += n_trials[target]\n",
    "print(\"combined accuracy:\", 100.*numerator/denominator, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric #2: Decoder in Correct Direction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps target to decoder position\n",
    "target2pos = {\n",
    "    0 : [-decoder_thresh, 0],  # left\n",
    "    1 : [decoder_thresh, 0],   # right\n",
    "    2 : [0, -decoder_thresh],  # down\n",
    "    3 : [0, decoder_thresh]    # up\n",
    "}\n",
    "\n",
    "# stores total number of decoded steps during each target\n",
    "n_active_steps = defaultdict(lambda: -1)\n",
    "correct_steps = defaultdict(lambda: 0)\n",
    "# threshold the decoder\n",
    "\n",
    "\n",
    "# compute per-class accuracy\n",
    "decoder_pos = defaultdict(lambda: 0)\n",
    "verbose = True  # prints intermediate values\n",
    "decoder_acc = defaultdict(lambda: 0)\n",
    "for target in targets:\n",
    "    decoder_pos[target] = np.sign(eeg_trials[target])\n",
    "    correct_steps[target] = decoder_pos[target] == np.sign(target2pos[target])  # signs are the same when it is closest\n",
    "    correct_steps[target] = np.sum(correct_steps[target], axis=-1)\n",
    "    correct_steps[target] = np.where(correct_steps[target]==2)[0].shape[0]\n",
    "    \n",
    "    # total number of decoded steps\n",
    "    n_trials = decoder_pos[target].shape[0] \n",
    "    n_tsteps_per_trial = decoder_pos[target].shape[1]\n",
    "    n_active_steps[target] = n_trials * n_tsteps_per_trial\n",
    "    \n",
    "    acc = correct_steps[target]*100./n_active_steps[target]\n",
    "    decoder_acc[target] = np.maximum(0., acc)  # negative accuracy indicates unknown class\n",
    "    if verbose:\n",
    "        print(\"\\nTarget:\", target)\n",
    "        print(\"total number of steps during this target:\", n_active_steps[target])\n",
    "        print(\"number of correct steps:\", correct_steps)\n",
    "        print(\"accuracy(%):\", acc )\n",
    "    \n",
    "# decoder_acc[target_ix] = accuracy_of_decoder_for_that_target\n",
    "\n",
    "# compute combined decoder accuracy\n",
    "numerator = 0\n",
    "denominator = 1e-12\n",
    "for target in targets:\n",
    "    numerator += correct_steps[target]\n",
    "    denominator += n_active_steps[target]\n",
    "print(\"combined accuracy:\", 100.*numerator/denominator, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metric #3: Decoder has Correct Velocity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# maps target to decoder position\n",
    "target2pos = {\n",
    "    0 : [-decoder_thresh, 0],  # left\n",
    "    1 : [decoder_thresh, 0],   # right\n",
    "    2 : [0, -decoder_thresh],  # down\n",
    "    3 : [0, decoder_thresh]    # up\n",
    "}\n",
    "\n",
    "# stores total number of decoded steps during each target\n",
    "n_active_steps = defaultdict(lambda: -1)\n",
    "\n",
    "# threshold the decoder\n",
    "\n",
    "\n",
    "# compute per-class accuracy\n",
    "decoder_pos = defaultdict(lambda: 0)\n",
    "verbose = True  # prints intermediate values\n",
    "decoder_acc = defaultdict(lambda: 0)\n",
    "for target in targets:\n",
    "    decoder_pos[target] = np.diff(eeg_trials[target], axis=-2) # take difference along the time axisy\n",
    "    decoder_pos[target] = np.sign(decoder_pos[target])   # signed velocity\n",
    "    correct_steps = decoder_pos[target] == np.sign(target2pos[target])  # signs are the same when moving towards target\n",
    "    correct_steps = np.sum(correct_steps, axis=-1)\n",
    "    correct_steps = np.where(correct_steps==2)[0].shape[0]\n",
    "    \n",
    "    # total number of decoded steps\n",
    "    n_trials = decoder_pos[target].shape[0] \n",
    "    n_tsteps_per_trial = decoder_pos[target].shape[1]\n",
    "    n_active_steps[target] = n_trials * n_tsteps_per_trial\n",
    "    \n",
    "    acc = correct_steps*100./n_active_steps[target]\n",
    "    decoder_acc[target] = np.maximum(0., acc)  # negative accuracy indicates unknown class\n",
    "    if verbose:\n",
    "        print(\"\\nTarget:\", target)\n",
    "        print(\"total number of steps during this target:\", n_active_steps[target])\n",
    "        print(\"number of correct steps:\", correct_steps)\n",
    "        print(\"accuracy(%):\", acc )\n",
    "    \n",
    "# decoder_acc[target_ix] = accuracy_of_decoder_for_that_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.sign(0) == np.sign(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tsteps_per_trial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.array(eeg_trials[target]).shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accuracy Metrics for 2D Decoding "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data\n",
    "data_file_name = \"jn1_replay_updates\"\n",
    "fpath = \"C:/Users/DNNeu/KaoLab/EEG_Decoding/Offline_EEG_Decoding/data/\" + data_file_name\n",
    "eeg_data = util.load_data(fpath + \"/eeg.bin\")\n",
    "task_data = util.load_data(fpath + \"/task.bin\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# partition the data into trials\n",
    "# extract trial start times\n",
    "new_state_bool = np.zeros(task_data['state_task'].size, dtype='bool')\n",
    "new_state_bool[1:] = (task_data['state_task'][1:] != task_data['state_task'][:-1])\n",
    "new_state_inds = np.nonzero(new_state_bool)[0]\n",
    "trial_labels = task_data['state_task'][new_state_inds]\n",
    "trial_start_times = task_data['time_ns'][new_state_inds] \n",
    "eeg_timing = eeg_data['time_ns']\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "eeg_trials = defaultdict(lambda: [])  \n",
    "# partition trials\n",
    "for trial_ix in range(len(trial_labels)-1):\n",
    "    if int(trial_labels[trial_ix]) == 4:  # skip rest trials\n",
    "        continue \n",
    "\n",
    "    # extract eeg_data indices were trial started and ended\n",
    "    trial_start_ix = np.where(task_data[\"time_ns\"] > trial_start_times[trial_ix])[0][0] + 50 # omit first second of data\n",
    "    trial_end_ix = np.where(task_data[\"time_ns\"] < trial_start_times[trial_ix+1])[0][-1]\n",
    "    \n",
    "    # get the decoder position\n",
    "    eeg_trials[trial_labels[trial_ix]].append(task_data[\"decoded_pos\"][trial_start_ix:trial_end_ix])\n",
    "\n",
    "    # append data from 64 electrodes\n",
    "    #eeg_trials.append(eeg_data['databuffer'][trial_start_ix:trial_end_ix, :64])\n",
    "    #min_trial_len = np.minimum(eeg_trials[-1].shape[0], min_trial_len).astype(int)\n",
    "    #eeg_trial_labels.append(class_names[trial_labels[trial_ix]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of targets: 2\n",
      "targets: [0, 1]\n"
     ]
    }
   ],
   "source": [
    "targets = np.unique(task_data[\"state_task\"]) \n",
    "targets = list(filter(lambda x: x != 4, targets))  # remove resting state\n",
    "n_targets = len(targets)\n",
    "print(\"number of targets:\", n_targets)\n",
    "print(\"targets:\", targets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute Weak Accuracy Metric for 2D Decoding\n",
    "\n",
    "The accuracy is computed for the 2D decoding task (left/right/up/down). A trial is considered accurate as long as the decoded cursor position ends on the correct half of the screen. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Target: 0\n",
      "correct trials: 15\n",
      "trials of this type: 76\n",
      "accuracy(%): 19.736842105263158\n",
      "\n",
      "Target: 1\n",
      "correct trials: 59\n",
      "trials of this type: 78\n",
      "accuracy(%): 75.64102564102564\n",
      "\n",
      "\n",
      "Average accuracy across all trials(%): 48.05194805194805\n"
     ]
    }
   ],
   "source": [
    "# maps target label to decoder 2D (x,y) position\n",
    "target2pos = {\n",
    "    0 : [-1, 0],  # left\n",
    "    1 : [1, 0],   # right\n",
    "    2 : [0, -1],  # down\n",
    "    3 : [0, 1]    # up\n",
    "}\n",
    "\n",
    "# maps each target to a list of bools indicating if that trial is correct\n",
    "correct_trials = defaultdict(lambda: 0)  \n",
    "n_trials = defaultdict(lambda: 0)  # number of trials for each condition\n",
    "\n",
    "# list of decoder positions at the end of all trials\n",
    "# e.g. final_decoder_pos[0] is a list of (x,y)-coordinates \n",
    "# of the final decoder position at the end of each trial \n",
    "# associated with left (target=0) reaches\n",
    "final_decoder_pos = defaultdict(lambda: 0)\n",
    "decoder_acc = defaultdict(lambda: 0)\n",
    "for target in targets:\n",
    "    # eeg_trials[target] is a list with length equal to the number of target trials.\n",
    "    # Each array is a timeseries of 2D position of the decoder for that trial\n",
    "    final_decoder_pos[target] = np.array(np.sign(eeg_trials[target]))[:,-1,:]  # final x,y-position of decoder on each trial (n_trials, 2) \n",
    "    if target < 2: # L/R\n",
    "        correct_trials[target] = final_decoder_pos[target][:,0] == target2pos[target][0]  # signs are the same when it is closest\n",
    "    else: # U/D\n",
    "        correct_trials[target] = final_decoder_pos[target][:,1] == target2pos[target][1]  # signs are the same when it is closest\n",
    "    correct_trials[target] = np.sum(correct_trials[target])\n",
    "    #correct_steps = np.where(correct_steps==2)[0].shape[0]\n",
    "    \n",
    "    # total number of this type of trial\n",
    "    n_trials[target] = final_decoder_pos[target].shape[0] \n",
    "    \n",
    "    # accuracy on this trial type\n",
    "    acc = correct_trials[target]*100./n_trials[target]\n",
    "    decoder_acc[target] = np.maximum(0., acc)  # negative accuracy indicates unknown class\n",
    "    if True:\n",
    "        print(\"\\nTarget:\", target)\n",
    "        print(\"correct trials:\", correct_trials[target])\n",
    "        print(\"trials of this type:\", n_trials[target])\n",
    "        print(\"accuracy(%):\", acc )\n",
    "\n",
    "total_correct_trials = 0\n",
    "total_trials = 0\n",
    "for target in targets:\n",
    "    total_correct_trials += correct_trials[target]\n",
    "    total_trials += n_trials[target]\n",
    "acc = total_correct_trials * 100. / total_trials\n",
    "print(\"\\n\\nAverage accuracy across all trials(%):\", acc)\n",
    "    \n",
    "# decoder_acc[target_ix] = accuracy_of_decoder_for_that_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
